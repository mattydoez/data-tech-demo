version: "3.9"

services:
  airflowdb:
    image: postgres:13
    ports:
      - "5433:5432"
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d airflow"]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s

  company_dw:
    image: postgres:13
    ports:
      - "5435:5432"
    environment:
      POSTGRES_DB: company_dw
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d company_dw"]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s
    volumes:
      - company_dw_data:/var/lib/postgresql/data
      - ./postgres_datawarehouse/postgresql.conf:/etc/postgresql/postgresql.conf
    command: ["postgres", "-c", "config_file=/etc/postgresql/postgresql.conf"]

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping"]
      interval: 30s
      timeout: 30s
      retries: 3

  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@company_dw:5432/company_dw
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
    entrypoint: ["/entrypoint.sh", "db", "init"]
    depends_on:
      airflowdb:
        condition: service_healthy
      redis:
        condition: service_healthy

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@company_dw:5432/company_dw
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      AIRFLOW__CORE__LOGGING_LEVEL: "INFO"
      AIRFLOW__WEBSERVER__BASE_URL: "http://localhost:8080"
      AIRFLOW__LOGGING__LOG_URL: "http://localhost:8080/log/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}.log"
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./data:/opt/airflow/data
    command: ["scheduler"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 10s
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - default

  airflow-worker:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@company_dw:5432/company_dw
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./data:/opt/airflow/data
    healthcheck:
      test: ["CMD-SHELL", "python /opt/airflow/scripts/worker_health_check.py"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 10s
    command: >
      bash -c "
      airflow celery worker --concurrency=1
      "
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
      airflow-scheduler:
        condition: service_healthy
    networks:
      - default
  
  airflow:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    environment:
      LOAD_EX: n
      EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: GmhP3ADRHscUZ2z_ohwMOmXlu5jFSI5IQRG0s-KrV_Y=
      AIRFLOW__WEBSERVER__SECRET_KEY: G8ZbXirwKyqoV6DvfotU-qIQrTB77iW5shFBR4L3PKk
      AIRFLOW__WEBSERVER__JWT_SECRET: zKyRQkchRLhbv4DzXXra5Pbq6jJ-3aZG6SmLWqFCSKw
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW_CONN_COMPANY_DW: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@company_dw:5432/company_dw
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@airflowdb:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: False
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__LOGGING_LEVEL: "INFO"
      AIRFLOW__WEBSERVER__BASE_URL: "http://localhost:8080"
      AIRFLOW__LOGGING__LOG_URL: "http://localhost:8080/log/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}.log"
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./data:/opt/airflow/data
    ports:
      - "8080:8080"
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username ${AIRFLOW_WWW_USER_USERNAME} --password ${AIRFLOW_WWW_USER_PASSWORD} --firstname FIRSTNAME --lastname LASTNAME --role Admin --email admin@example.com &&
      exec airflow webserver"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-scheduler:
        condition: service_healthy
      airflow-worker:
        condition: service_healthy
      redis:
        condition: service_healthy


  superset_db:
    image: postgres:13
    ports:
      - "5434:5432"
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d superset"]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s
    volumes:
      - superset_db_data:/var/lib/postgresql/data

  superset:
    build:
      context: ./superset
      dockerfile: Dockerfile
    environment:
      SUPERSET_ADMIN: ${SUPERSET_ADMIN}
      SUPERSET_PASSWORD: ${SUPERSET_PASSWORD}
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@superset_db:5432/superset
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
    command: /app/init_superset.sh
    depends_on:
      - superset_db

networks:
  default:
    driver: bridge

volumes:
  superset_db_data:
  superset_home:
  company_dw_data: